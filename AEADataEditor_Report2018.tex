% AEJ-Article.tex for AEA last revised 22 June 2011
\documentclass[AEJ]{AEA}

%%%%%% NOTE FROM OVERLEAF: The mathtime package is no longer publicly available nor distributed. We recommend using a different font package e.g. mathptmx if you'd like to use a Times font.
% \usepackage{mathptmx}

% The mathtime package uses a Times font instead of Computer Modern.
% Uncomment the line below if you wish to use the mathtime package:
%\usepackage[cmbold]{mathtime}
% Note that miktex, by default, configures the mathtime package to use commercial fonts
% which you may not have. If you would like to use mathtime but you are seeing error
% messages about missing fonts (mtex.pfb, mtsy.pfb, or rmtmi.pfb) then please see
% the technical support document at http://www.aeaweb.org/templates/technical_support.pdf
% for instructions on fixing this problem.

% Note: you may use either harvard or natbib (but not both) to provide a wider
% variety of citation commands than latex supports natively. See below.

% Uncomment the next line to use the natbib package with bibtex 
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{acronym}
\usepackage[names]{xcolor}
\usepackage{graphicx}
% Uncomment the next line to use the harvard package with bibtex
%\usepackage[abbr]{harvard}
\usepackage{etoolbox}
\newtoggle{fancy}
\togglefalse{fancy}

%%%
%%% TESTING
%%%
%\usepackage{blindtext}

\usepackage{textcomp}

\iftoggle{fancy}{
\input{fancy-config.tex}
}{}

% This command determines the leading (vertical space between lines) in draft mode
% with 1.5 corresponding to "double" spacing.
\draftSpacing{1.5}

%% Acronyms
\acrodef{AEA}{American Economic Association}
\acrodef{DOI}{Digital Object Identifier}
\acrodef{FAIR}{Findable, Accessible, Interoperable, Re-usable}
\acrodef{PSID}{Panel Study of Income Dynamics}
\acrodef{HRS}{Health and Retirement Study}

% reset colors
\definecolor{darkblue}{rgb}{0 0 255}
\hypersetup{colorlinks,breaklinks,citecolor=darkblue,linkcolor=darkblue,urlcolor=darkblue}



\begin{document}

\title{Report for 2018 by the AEA Data Editor }
\shortTitle{Report by Data Editor}
\author{Lars Vilhuber\thanks{%
Vilhuber: Cornell University, lars.vilhuber@cornell.edu.}}
\date{\today}
\pubMonth{Month}
\pubYear{Year}
\pubVolume{Vol}
\pubIssue{Issue}
\JEL{}
\Keywords{reproducibility; replicability; science of science}




\maketitle


The purpose of scientific publishing is the dissemination of robust research findings, exposing them to the scrutiny of peers. Key to this endeavor is documenting the provenance of those findings. For theoretical articles, these are the proofs of theorems and the like that the authors provide. For empirical articles, the foundations on which the findings reside are external to the article, and often to the journal, in which they are published. Many scientists,  journals, learned societies, and funding agencies have called for greater transparency of research practices, and more assurance that published research is reproducible \citep{Stodden2016-uc,Fuentes2016-wz,Moffitt2016-wl,Camerer2016-kl,Bollen2015-vb,Joskow2015-hd,ChristensenTransparencyReproducibilityCredibility2018}. Our scientific community faces  increasingly complex issues of privacy and confidentiality that prevent ``open'' access to those same sources \citep{Anderson12009,abowdschmutte.aer.2018}. Large and private databases (often both at the same time) are being used to analyze economic phenomena, with subsequent publications \citep{BakerWagePolicyFirm1994,LazearAm.Econ.Rev.2000,BaileySocialConnectednessMeasurement2018,Chen2017,HallILRReview2018}, yet few such data are available for replication exercises \cite{JengAm.Econ.Rev.2016}. To ensure the credibility of the scientific endeavor, transparency of the methods and data used are critical. Various studies have shown that too few studies are (easily) reproducible \citep{McCullough2007-zx,McCullough2006-cz,AndersonJ.Econ.Methodol.2008,AndersonFed.ReserveBankStLouisRev.1994}. There is a need to properly cite the digital inputs to our published output and to properly curate those inputs.  

In January 2018, I was appointed as the first Data Editor of the American Economic Association, with the mission to ``design  and  oversee  the  AEA  journalsâ€™  strategy for archiving and curating research data and promoting  reproducible  research'' \citep{10.1257/pandp.108.745}. This first report by a Data Editor describes my efforts over the past year to advance  that mission. It also highlights some of the short- and medium-term changes that economists might expect when publishing their research.\footnote{A variety of replication concepts have been defined in economics \citep{Hamermesh2007,ClemensJ.Econ.Surv.2017}. In this article, we adopt the definitions articulated by \citet{Bollen2015-vb}, among others. \textit{Reproducibility}  refers to ``the ability [$\dots$] to duplicate the results of a prior study using the same materials and procedures as were used by the original investigator,'' and is related to the ``narrow" sense of replication of \cite{Pesaran2003}. Use of the ``same procedures'' may imply using the same computer code or re-implementing the statistical procedures in a different software package. \cite{Hamermesh2007} calls this ``pure replication". \citet[p. 942]{ChristensenTransparencyReproducibilityCredibility2018} argue that this is the ``basic standard [that] should be expected of all published economics research, and hope this expectation is universal among researchers.'' \textit{Replicability} refers to ``the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected'' \citep[: ``wider'' sense of replication]{Pesaran2003}, while \textit{generalizability} refers to the extension of the scientific findings to other populations, contexts, and time frames, perhaps using different methods \citep[: ``scientific replication'']{Hamermesh2017}}


\section{The current environment}

The \ac{AEA}'s data and code posting policy \citep{American_Economic_Association2008-az}, as well as that of other societies and journals, are a reaction to earlier calls to increase transparency \citep{McCullough2006-cz,AndersonJ.Econ.Methodol.2008}, and are intended to create a minimal framework from which to replicate empirical findings, by requiring the data and code to be available to others. In practice, enough reproduction and replication attempts fail \citep{CamererScience2016,Chang2015-dl,ChangAm.Econ.Rev.2017}, not just in economics \citep{Baker2015-sh,Collaboration2015-ev} (I will comment on our own efforts later). It remains an open question who should be tasked with conducting a ``replication'' in the first place - should the editorial team verify reproducibility during the editorial process \citep{JacobyInsideHigherEd2017}, should the referees be able to do this, or should they be required to do this? Or should the readers of the articles, and the broader scientific community, attest to the replicability and ultimately the generalizability of the findings \citep{Hamermesh2017}? Related is the question whether  enough replications are being published \citep{BerryAm.Econ.Rev.2017,Burman2010-ng,Coffman2017-si,Duvendack2017-js,Hoffler-LibMag.2017}.

Very few journals have implemented verification of submitted code and data during the editorial process. In political science, the American Journal of Political Science in collaboration with the Odum Institute for Research in Social Science \citep{Christian2018} has been conducting data curation and code verification. The Journal of the American Statistical Association performs a ``broad evaluation of quality and potential for usability of the code and data'' since 2016 \citep{Stodden2016-uc}.

%%%% DATA ARCHIVE LISTS
%highlight the verification on data archives \citep{Open_Science_Framework2017-zc}, maintain lists of acceptable third-party repositories,%
%\footnote{Nature Scientific Data maintains a list for its journals \citep{Nature_Scientific_Data2016-hl}, and other institutions (CoreTrustSeal, FAIRsharing) have as their primary purpose to perform this kind of vetting.} 
%and interlink with collaborating repositories to highlight authors' (and repositories') contributions to the data component of a scholarly work.%
%\footnote{Elsevier interlinks, for instance, with ICPSR, highlighting the use of a repository on the article's web page.}

%%% EDUCATION
%Outside of journals, several projects are working to educate the community to incorporate principles of reproducibility and traceability into their workflow.%
%\footnote{Open Science Framework, Project TIER, BITSS are just a few of those active in that field \citep{Gentzkow2014-va,Wilson2016-bt}.}

No journal currently does an adequate job of providing information about restricted-access data.\footnote{Elsevier journals have experimented with "Data Descriptions", but while the form is machine-readable, it is essentially free-form text, and checking the box "confidential data" essentially stops the process of filling in any information.} This is not only the fault of the journals: Most restricted-access data centers do not provide structured information about existence, modalities of access, or even data landing pages for the datasets they provide access to.%
\footnote{Restricted-access data hosted on ICPSR and possibly Harvard Dataverse are notable exceptions.} 
None of these solutions are widespread, and standards are only now being developed.

License: \cite{StoddenSoftwarePatentsBarrier2012}


%\include{cornerbox.tex}
\section{The Mission, if You Choose to Accept It}
With the mission outlined above in mind, the Data Editor's long-term tasks are
\begin{enumerate}
	\item Elaborate a data and code availability policy that is modern, responsive, and imposes the lowest burden on authors and readers that is commensurate with the overall goals;
	\item Creating technical, human, and organizational infrastructure at the AEA journals to support all aspects of implementing the data and code availability policy;
	\item Working with other providers of scientific infrastructure to improve support for documenting provenance and replicability;
	\item Working with the economics community to enhance and broaden education on replicable science;
	\item Conducting research and participating in experiments in the intersection of publication, replication, and provenance documentation
\end{enumerate}
In particular, a revised data and code posting policy should maximize credibility and trustworthiness of research findings, and address the following goals: 
\begin{enumerate}
	\item to encourage and reward incorporating basic principles of replicability into researchers' workflow;
	\item to prioritize linking to existing data and code repositories, as the primary mechanism of providing source materials, with a journal-sanctioned repository as a fall-back archive;
	\item to require and facilitate proper documentation of  restricted-access data;
	\item to enforce a limited measure of verification;
	\item balance the previous goals with the need to \textit{reduce} the burden on authors, not increase it. 
\end{enumerate}

\section{Implementing improved transparency of research}
In the first year, we have moved a few tasks forward. 

A modern data and code availability policy should support both reproducibility and replicability, by supporting accurate and transparent description of the provenance of the scientific results. In particular, a functional implementation of those concepts suggests that both data and code need to be subject to the \ac{FAIR} principles \cite{FORCE11FAIRDATAPRINCIPLES}: findable, accessible, interoperable, and re-usable. In this context, we interpret the ``interoperability'' of code as ``code that works, and the workings of which are comprehensible by a third party'' (CITE??). 

\subsection{Goal 1: Improved findability of data used in research articles}

Under this goal, we start by abolishing journal-specific ``supplementary materials'' as the primary repository of data and code that are part of the provenance chain of an article. As currently implemented at most journals, including the \ac{AEA}'s journals, they lack findability, proper citability as first-class objects, and are somewhat opaque (packaged as ZIP files). Historical materials will be migrated to a new curated archive at ICPSR. The AEA's ``Data and Code Archive @ ICPSR'' will display the full contents of the materials as deposited by authors in the past, without the need to download ZIP files. The materials will receive their own citable \ac{DOI}. Through the \ac{DOI} registrars, we automatically leverage the ability to link and associate the archives with their original articles. 

On the AEA's journal websites, the links to ``supplementary'' materials will initially appear to be the same (although pointing to the new locations), but future enhancements will allow for greater visibility or transparency of the associated materials. However, by separating the hosting of data archives (for historical materials, at ICPSR) from the referencing of those archives, we open the door to a more consistent model of linking to and citing data artifacts associated with published articles.

For future submissions, we will allow researchers to reference supplementary materials on a wide list of data archives or repositories.\footnote{Do I need to distinguish the two? is there a difference?} 
In particular, while some journals are already curating a list of recommended \textit{open} data archives \citep{Nature_Scientific_Data2016-hl} (ALSO PLOS, F1000), we will also allow authors to reference materials in reliable \textit{restricted-access} data repositories. What constitutes a ``reliable'' data repository? For one, it needs \textit{persistence}, and \textit{accessibility}. Properly managed repositories have a \textit{preservation policy} - they commit to maintaining deposits for a defined duration (often, but not always, in perpetuity), and only under very restrictive circumstances will remove deposits. Such repositories will also have a policy about access - who can obtain data deposited at the institution, and under what conditions. This characterization applies homogeneously to open and restricted-access repositories. A recommended repository will have been vetted (by the AEA Data Editor, or a reliable third party) to have acceptable and credible policies. 

Authors submitting their work to the AEA journals will be affected in several ways. First, those authors who already deposit their (open access) code and data at known repositories will not have to do so again - a simple reference (and citation!) of the previously archived materials is sufficient. Authors who use data provided through institutional providers (\ac{PSID}, \ac{HRS}, the U.S. Census Bureau, and international equivalents), and who in some cases cannot deposit the data, will also reference the persistent location where they obtained their data from, and where others can do so as well. In the case of restricted-access, a better description of access procedures will be requested from authors, who in turn should ask their data providers to provide such procedural descriptions, in the form of web pages and (persistent and citable) documents. 



If replicability is truly part of the research scientist's workflow, then by the time she submits an article to any journal, the intermediate and final data products as well as the code used for an article have already been deposited at appropriate repositories and archives. If all such repositories and archives are of sufficient quality, then the additional deposit at a journal is duplicative at best, and perturbative to the provenance chain at worst. The right solution is to reference those other repositories, not copy them. Of course, for those that have not used repositories and simply wish to provide a replication zip archive of the files on their laptop, an adequate deposit solution should also exist.
By fundamentally relying on references to repositories instead of deposits, it also becomes possible to put public-use and restricted-access data on a comparable footing at the journal with regards to potential replicability. Well documented location of data (through DOI), access protocols (implicit or click-through license, contracts, etc.), and access mechanism (direct download, delivery of physical media or controlled download, sign-on to controlled secure access, etc.) are then available for any data.

\section{Goal 2: Improved Reliability of Replication Materials}
\begin{figure}
    \centering
\includegraphics[width=0.7\textwidth]{images/aer_programs_by_year.png}
    \caption{Popularity of statistical software in the AER}
    \label{fig:aer_programs_by_year}
    \footnotesize Figure provided by Patrick Baylis (UBC), based on filename extensions in ZIP files of replication materials on the AEA website.
\end{figure}

\cite{ChristianOperationalizingReplicationStandard2018}


\section{Data Citations}
Properly referencing data goes beyond just reproducibility - it is also proper scientific writing style. In the same way that we use bibliographic references to ``printed'' resources, we should also be using such references for data resources, to give and receive credit where credit is due. Not referencing an article or book is at best an oversight, and at worst plagiarism - and the same should apply to data objects. Numerous guides and tutorials exist  \citep{dataone-l09,icpsr-data-cite,force11declaration}.

The AEA uses the Chicago style for citations and bibliographies \citep{aeadatarefs}. However, the Chicago Style Manual \citep{citation-machine,ChicagoManualofStyleChicagoManualStyle2018} does not provide examples for data citations, and neither does the Citation Style Language\footnote{\url{https://citationstyles.org/}} used by applications like Zotero\footnote{\url{https://www.zotero.org/}} and Mendeley Desktop\footnote{\url{https://www.mendeley.com/download-desktop/}}.


As part of our activities, the AEA prepress department has started the process of updating AEA templates available through such software.\footnote{For the technically inclined, this process involves updating an existing style or creating a new style on \url{https://citationstyles.org/} and \url{https://github.com/citation-style-language/styles}, from where it propagates to a large number of software packages.} Some guidance for data citations is provided at 




\section{Future activities}
Keeping an eye on novel techniques \cite{BrinckmanComputingenvironmentsreproducibility2018,Butlerroledatasupplements2018}
REgistered reports: \cite{ChrisChambersRegisteredReportsstep2014,NosekRegisteredReportsMethod2014}

% Remove or comment out the next two lines if you are not using bibtex.
\bibliographystyle{aea}
\bibliography{paper,references,zotero,aej-rep}

% The appendix command is issued once, prior to all appendices, if any.
\appendix

\section{Reviewers}

\end{document}

